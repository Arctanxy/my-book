随着爬虫技术越来越普及，特别是Python爬虫相关的第三方库越来越完善，即便是Python初学者也能够通过寥寥几行代码写出一个小爬虫。

所以网站管理者不得不为自己的网站添加各种反爬取手段，将这些个人小爬虫拒之门外（百度蜘蛛这样的爬虫他们还是欢迎的）。

要编写出高效且稳定的爬虫，必须了解网站的各种反爬取手段的原理。

1. 识别爬虫标签

2. 登录验证

3. 验证码

4. 爬取间隔与次数限制

采取爬取间隔和次数限制的网站一般会禁止数据异常的IP发出的请求。


反反爬虫手段

1. cookie

2. 识别验证码

2.1 python第三方库识别或机器学习识别

2.2 人工打码
像谷歌搜索的验证码、12306的验证码，人工识别都费劲，想用简单几行代码就识别出来基本不可能，可以选择人工打码的方式，也就是购买人工打码服务，雇人替你手动识别验证码。
3. 控制爬取频率

4. 建立IP池

5. selenium

