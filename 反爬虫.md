随着爬虫技术越来越普及，特别是Python爬虫相关的第三方库越来越完善，即便是Python初学者也能够通过寥寥几行代码写出一个小爬虫。

所以网站管理者不得不为自己的网站添加各种反爬取手段，将这些个人小爬虫拒之门外（百度蜘蛛这样的爬虫他们还是欢迎的）。

要编写出高效且稳定的爬虫，必须了解网站的各种反爬取手段的原理。

1. 识别爬虫标签

有些网站受爬虫影响比较小，管理者也比较懒，会在接受请求时验证一下user-agent，挡一下最简陋的小爬虫。

2. 登录验证

3. 验证码

4. 爬取间隔与次数限制

这种反爬虫手段是基于爬虫与正常人类浏览网站的行为不同来辨别爬虫的，采取这种策略的网站通常会封掉爬取频率过高或者一段时间内访问总次数过高的爬虫。


反反爬虫手段

1. 伪装成浏览器

2. 识别验证码

2.1 python第三方库识别或机器学习识别

2.2 人工打码

像谷歌搜索的验证码、12306的验证码，人工识别都费劲，想用简单几行代码就识别出来基本不可能，可以选择人工打码的方式，也就是购买人工打码服务，雇人替你手动识别验证码。此方法用来应对验证码效果很好。

3. 控制爬取频率

4. 建立代理IP池
百度搜索代理IP，就能找到很多的代理IP网站，提供大量的免费代理IP，如果不想花钱的话，可以用爬虫把这些免费代理IP收集起来，批量筛选验证一下，留作日后编写爬虫之用。
但是免费代理IP因为都是公开的，所以某些勤奋的网站开发者也会没事写个爬虫爬取免费代理IP，然后把这些公开的免费代理IP加入黑名单……所以当你更换代理IP之后还是频频被禁，就改想想是不是要花点钱购买一些高质量的代理IP了。

5. selenium

